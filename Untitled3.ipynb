{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3TIygXC5hnBe97tAO7fSH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hosein9574/My-agents/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlrvbVVXBA9y"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install transformers --upgrade\n",
        "!pip install sentence-transformers --upgrade\n",
        "!pip install chromadb --upgrade\n",
        "!pip install gradio --upgrade\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from datetime import datetime\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import gradio as gr\n",
        "\n",
        "# Check versions\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "print(\"Sentence-Transformers version:\", sentence_transformers.__version__)\n",
        "print(\"ChromaDB version:\", chromadb.__version__)\n",
        "\n",
        "# Set up Kaggle API (make sure kaggle.json is uploaded in Colab first)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download and unzip BBC News dataset\n",
        "!kaggle datasets download -d gpreda/bbc-news\n",
        "!unzip -o bbc-news.zip\n",
        "\n",
        "# Load data\n",
        "news = pd.read_csv('./bbc_news.csv')\n",
        "MAX_NEWS = 1000\n",
        "DOCUMENT = \"description\"\n",
        "TOPIC = \"title\"\n",
        "news[\"id\"] = news.index\n",
        "subset_news = news.head(MAX_NEWS)\n",
        "\n",
        "# Setup ChromaDB\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chromadb\")\n",
        "collection_name = \"news_collection_\" + datetime.now().strftime(\"%s\")\n",
        "\n",
        "# Safe check for existing collection\n",
        "existing_names = [col.name for col in chroma_client.list_collections()]\n",
        "if collection_name in existing_names:\n",
        "    chroma_client.delete_collection(name=collection_name)\n",
        "\n",
        "collection = chroma_client.create_collection(name=collection_name)\n",
        "\n",
        "# Embed and add to ChromaDB\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embeddings = embedding_model.encode(subset_news[DOCUMENT].tolist(), convert_to_numpy=True)\n",
        "\n",
        "collection.add(\n",
        "    documents=subset_news[DOCUMENT].tolist(),\n",
        "    metadatas=[{TOPIC: topic} for topic in subset_news[TOPIC].tolist()],\n",
        "    ids=[f\"id{x}\" for x in range(MAX_NEWS)],\n",
        "    embeddings=embeddings.tolist(),\n",
        ")\n",
        "\n",
        "# Load language model\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "lm_model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=lm_model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    device_map=\"auto\",  # Use GPU if available\n",
        ")\n",
        "\n",
        "# QA function\n",
        "def answer_question(user_question):\n",
        "    try:\n",
        "        results = collection.query(query_texts=[user_question], n_results=5)\n",
        "        context = \"\\n\".join(results[\"documents\"][0])\n",
        "        context = context[:5120]  # Truncate if needed\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Relevant context: {context}\n",
        "        Considering the relevant context, answer the question.\n",
        "        Question: {user_question}\n",
        "        Answer: \"\"\"\n",
        "\n",
        "        response = pipe(prompt)\n",
        "        answer = response[0][\"generated_text\"].split(\"Answer:\")[-1].strip()\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "# Gradio Chat Interface\n",
        "def chat_function(message, history):\n",
        "    return answer_question(message)\n",
        "\n",
        "gr.ChatInterface(fn=chat_function, title=\"ðŸ“° News QA Bot\", description=\"Ask anything about recent news articles.\").launch()"
      ]
    }
  ]
}